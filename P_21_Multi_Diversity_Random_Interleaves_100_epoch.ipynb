{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a55b1f0-82ab-4922-b781-625766c4e7b9",
      "metadata": {
        "id": "7a55b1f0-82ab-4922-b781-625766c4e7b9",
        "outputId": "bdfcfe06-3956-47fb-d46c-914e9aae1fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in ./lib/python3.10/site-packages (2.4.4)\n",
            "Requirement already satisfied: nibabel in ./lib/python3.10/site-packages (5.2.0)\n",
            "Requirement already satisfied: numpy in ./lib/python3.10/site-packages (1.26.4)\n",
            "Requirement already satisfied: torch in ./lib/python3.10/site-packages (2.2.0)\n",
            "Requirement already satisfied: torchvision in ./lib/python3.10/site-packages (0.17.0)\n",
            "Requirement already satisfied: segmentation-models-pytorch in ./lib/python3.10/site-packages (0.3.3)\n",
            "Requirement already satisfied: scikit-learn in ./lib/python3.10/site-packages (1.4.0)\n",
            "Requirement already satisfied: packaging>=17 in ./lib/python3.10/site-packages (from nibabel) (23.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./lib/python3.10/site-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: fsspec in ./lib/python3.10/site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: networkx in ./lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: triton==2.2.0 in ./lib/python3.10/site-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: sympy in ./lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: jinja2 in ./lib/python3.10/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: filelock in ./lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: requests in ./lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in ./lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: timm==0.9.2 in ./lib/python3.10/site-packages (from segmentation-models-pytorch) (0.9.2)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in ./lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.4)\n",
            "Requirement already satisfied: tqdm in ./lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.2)\n",
            "Requirement already satisfied: munch in ./lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n",
            "Requirement already satisfied: huggingface-hub in ./lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.20.3)\n",
            "Requirement already satisfied: pyyaml in ./lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: safetensors in ./lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.10/site-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.10/site-packages (from requests->torchvision) (2.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pydicom nibabel numpy torch torchvision segmentation-models-pytorch scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72e24cf-4548-4014-b732-434bb08f3b99",
      "metadata": {
        "id": "c72e24cf-4548-4014-b732-434bb08f3b99",
        "outputId": "1d798aa3-f3fc-47af-e223-bef8459d0e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in ./lib/python3.10/site-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in ./lib/python3.10/site-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in ./lib/python3.10/site-packages (from albumentations) (1.12.0)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in ./lib/python3.10/site-packages (from albumentations) (0.22.0)\n",
            "Requirement already satisfied: PyYAML in ./lib/python3.10/site-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in ./lib/python3.10/site-packages (from albumentations) (4.9.0.80)\n",
            "Requirement already satisfied: qudida>=0.0.4 in ./lib/python3.10/site-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: typing-extensions in ./lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (4.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in ./lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (1.4.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in ./lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2024.2.12)\n",
            "Requirement already satisfied: networkx>=2.8 in ./lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.27 in ./lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2.34.0)\n",
            "Requirement already satisfied: packaging>=21 in ./lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (23.2)\n",
            "Requirement already satisfied: pillow>=9.0.1 in ./lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (10.2.0)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in ./lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259e95b5-55e5-49b0-846d-4520caffa2e1",
      "metadata": {
        "id": "259e95b5-55e5-49b0-846d-4520caffa2e1",
        "outputId": "52c44126-3ee5-44da-8c9a-fdcc8ec74a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in ./lib/python3.10/site-packages (2.2.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in ./lib/python3.10/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in ./lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0004d78c-3260-4376-9f89-6601f0f197a9",
      "metadata": {
        "id": "0004d78c-3260-4376-9f89-6601f0f197a9",
        "outputId": "1a958213-3f31-4e7d-9104-6dec1840603c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50] - Train IoU: 0.0892 - Validation IoU: 0.0602\n",
            "Epoch [2/50] - Train IoU: 0.1282 - Validation IoU: 0.1177\n",
            "Epoch [3/50] - Train IoU: 0.1814 - Validation IoU: 0.1518\n",
            "Epoch [4/50] - Train IoU: 0.2204 - Validation IoU: 0.1869\n",
            "Epoch [5/50] - Train IoU: 0.2857 - Validation IoU: 0.2272\n",
            "Epoch [6/50] - Train IoU: 0.3541 - Validation IoU: 0.2785\n",
            "Epoch [7/50] - Train IoU: 0.4238 - Validation IoU: 0.3346\n",
            "Epoch [8/50] - Train IoU: 0.4863 - Validation IoU: 0.3707\n",
            "Epoch [9/50] - Train IoU: 0.5346 - Validation IoU: 0.4270\n",
            "Epoch [10/50] - Train IoU: 0.5581 - Validation IoU: 0.4675\n",
            "Epoch [11/50] - Train IoU: 0.6064 - Validation IoU: 0.4980\n",
            "Epoch [12/50] - Train IoU: 0.6271 - Validation IoU: 0.5364\n",
            "Epoch [13/50] - Train IoU: 0.6349 - Validation IoU: 0.5732\n",
            "Epoch [14/50] - Train IoU: 0.6883 - Validation IoU: 0.6046\n",
            "Epoch [15/50] - Train IoU: 0.6723 - Validation IoU: 0.6157\n",
            "Epoch [16/50] - Train IoU: 0.7076 - Validation IoU: 0.6369\n",
            "Epoch [17/50] - Train IoU: 0.7318 - Validation IoU: 0.6641\n",
            "Epoch [18/50] - Train IoU: 0.7527 - Validation IoU: 0.6773\n",
            "Epoch [19/50] - Train IoU: 0.7676 - Validation IoU: 0.6830\n",
            "Epoch [20/50] - Train IoU: 0.7690 - Validation IoU: 0.6925\n",
            "Epoch [21/50] - Train IoU: 0.7844 - Validation IoU: 0.7102\n",
            "Epoch [22/50] - Train IoU: 0.7854 - Validation IoU: 0.7323\n",
            "Epoch [23/50] - Train IoU: 0.7936 - Validation IoU: 0.7482\n",
            "Epoch [24/50] - Train IoU: 0.8033 - Validation IoU: 0.7509\n",
            "Epoch [25/50] - Train IoU: 0.8189 - Validation IoU: 0.7612\n",
            "Epoch [26/50] - Train IoU: 0.8129 - Validation IoU: 0.7830\n",
            "Epoch [27/50] - Train IoU: 0.8298 - Validation IoU: 0.7978\n",
            "Epoch [28/50] - Train IoU: 0.8382 - Validation IoU: 0.8001\n",
            "Epoch [29/50] - Train IoU: 0.8363 - Validation IoU: 0.8042\n",
            "Epoch [30/50] - Train IoU: 0.8581 - Validation IoU: 0.8189\n",
            "Epoch [31/50] - Train IoU: 0.8699 - Validation IoU: 0.8302\n",
            "Epoch [32/50] - Train IoU: 0.8712 - Validation IoU: 0.8387\n",
            "Epoch [33/50] - Train IoU: 0.8696 - Validation IoU: 0.8425\n",
            "Epoch [34/50] - Train IoU: 0.8671 - Validation IoU: 0.8532\n",
            "Epoch [35/50] - Train IoU: 0.8705 - Validation IoU: 0.8626\n",
            "Epoch [36/50] - Train IoU: 0.8936 - Validation IoU: 0.8693\n",
            "Epoch [37/50] - Train IoU: 0.8935 - Validation IoU: 0.8727\n",
            "Epoch [38/50] - Train IoU: 0.9003 - Validation IoU: 0.8730\n",
            "Epoch [39/50] - Train IoU: 0.8975 - Validation IoU: 0.8742\n",
            "Epoch [40/50] - Train IoU: 0.8954 - Validation IoU: 0.8773\n",
            "Epoch [41/50] - Train IoU: 0.9034 - Validation IoU: 0.8811\n",
            "Epoch [42/50] - Train IoU: 0.9142 - Validation IoU: 0.8747\n",
            "Epoch [43/50] - Train IoU: 0.9120 - Validation IoU: 0.8733\n",
            "Epoch [44/50] - Train IoU: 0.9153 - Validation IoU: 0.8695\n",
            "Epoch [45/50] - Train IoU: 0.9149 - Validation IoU: 0.8765\n",
            "Epoch [46/50] - Train IoU: 0.9178 - Validation IoU: 0.8789\n",
            "Epoch [47/50] - Train IoU: 0.9215 - Validation IoU: 0.8810\n",
            "Epoch [48/50] - Train IoU: 0.9252 - Validation IoU: 0.8785\n",
            "Epoch [49/50] - Train IoU: 0.9269 - Validation IoU: 0.8766\n",
            "Epoch [50/50] - Train IoU: 0.9251 - Validation IoU: 0.8807\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import random\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from scipy.ndimage import zoom\n",
        "from operator import itemgetter\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "num_classes = 7\n",
        "batch_size = 8\n",
        "img_root = \"/home/ealam/JHIR_Hip_Knee_Datasets/Hip/Images\"\n",
        "mask_root = \"/home/ealam/JHIR_Hip_Knee_Datasets/Hip/Annotations\"\n",
        "metadata_path = \"/home/ealam/JHIR_Hip_Knee_Datasets/Hip/segmentation_with_racegender.csv\"\n",
        "test_augmentations = A.Compose([\n",
        "    A.Resize(height=256, width=256),\n",
        "    A.Normalize(mean=(0.485,), std=(0.229,)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "class MulticlassHipSegmentationDataset(Dataset):\n",
        "    def __init__(self, img_root, mask_root, metadata_df, paired_files, num_classes, transforms=None, preprocessing=None):\n",
        "        self.img_root = img_root\n",
        "        self.mask_root = mask_root\n",
        "        self.metadata_df = metadata_df\n",
        "        self.paired_files = paired_files\n",
        "        self.num_classes = num_classes\n",
        "        self.transforms = transforms\n",
        "        self.preprocessing = preprocessing\n",
        "        self.used_pairs = set()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paired_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_file, mask_file = self.paired_files[idx]\n",
        "        if not os.path.exists(os.path.join(self.mask_root, mask_file)):\n",
        "            return None\n",
        "\n",
        "        dicom_image = pydicom.dcmread(os.path.join(self.img_root, image_file))\n",
        "        image = dicom_image.pixel_array.astype(np.float32)\n",
        "        annotation = nib.load(os.path.join(self.mask_root, mask_file))\n",
        "        annotation_data = annotation.get_fdata()\n",
        "        if len(annotation_data.shape) == 3:\n",
        "            annotation_data = annotation_data[:, :, 0]\n",
        "\n",
        "        annotation_data = self.calculate_flipped_rotated_mask(annotation_data)\n",
        "\n",
        "        if annotation_data.ndim > 2 and annotation_data.shape[-1] != 1:\n",
        "            raise ValueError('Mask has multiple channels')\n",
        "\n",
        "        if image.shape != annotation_data.shape:\n",
        "            zoom_factors = np.array(image.shape) / np.array(annotation_data.shape)\n",
        "            annotation_data = zoom(annotation_data, zoom_factors, order=0)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=image, mask=annotation_data)\n",
        "            image = transformed[\"image\"]\n",
        "            annotation_data = transformed[\"mask\"]\n",
        "\n",
        "        annotation_data_onehot = self.one_hot_encode(annotation_data)\n",
        "\n",
        "        if self.preprocessing is not None:\n",
        "            transformed = self.preprocessing(image=image, mask=annotation_data_onehot)\n",
        "            image = transformed[\"image\"]\n",
        "            annotation_data_onehot = transformed[\"mask\"]\n",
        "\n",
        "        patient_id = int(float(image_file.split(\".\")[0]))\n",
        "        racegender_info = self.metadata_df.loc[self.metadata_df['id'] == patient_id]['racegender'].values\n",
        "        racegender = racegender_info[0] if racegender_info else 'Unknown'\n",
        "\n",
        "        return image, annotation_data_onehot, racegender\n",
        "\n",
        "    def one_hot_encode(self, mask):\n",
        "        one_hot_mask = np.zeros((self.num_classes, *mask.shape), dtype=np.float32)\n",
        "        for class_idx in range(self.num_classes):\n",
        "            one_hot_mask[class_idx][mask == class_idx] = 1.0\n",
        "        return one_hot_mask\n",
        "\n",
        "    def calculate_flipped_rotated_mask(self, mask):\n",
        "        rotated_mask = cv2.rotate(mask, cv2.ROTATE_90_CLOCKWISE)\n",
        "        flipped_rotated_mask = cv2.flip(rotated_mask, 1)\n",
        "        return flipped_rotated_mask\n",
        "\n",
        "metadata_df = pd.read_csv(metadata_path)\n",
        "\n",
        "image_files = sorted(os.listdir(img_root))\n",
        "mask_files = sorted(os.listdir(mask_root))\n",
        "\n",
        "paired_files = []\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_id = os.path.splitext(image_file)[0]\n",
        "    mask_file = f\"{image_id}.nii.gz\"\n",
        "    if mask_file in mask_files:\n",
        "        paired_files.append((image_file, mask_file))\n",
        "\n",
        "random.shuffle(paired_files)\n",
        "\n",
        "train_size = int(0.7 * len(paired_files))\n",
        "valid_size = int(0.1 * len(paired_files))\n",
        "test_size = len(paired_files) - train_size - valid_size\n",
        "\n",
        "train_pairs = paired_files[:train_size]\n",
        "valid_pairs = paired_files[train_size:train_size + valid_size]\n",
        "test_pairs = paired_files[train_size + valid_size:]\n",
        "\n",
        "train_set = MulticlassHipSegmentationDataset(\n",
        "    img_root, mask_root, metadata_df, train_pairs, num_classes,\n",
        "    transforms=test_augmentations\n",
        ")\n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet18\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=1,\n",
        "    classes=num_classes,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "valid_set = MulticlassHipSegmentationDataset(\n",
        "    img_root, mask_root, metadata_df, valid_pairs, num_classes,\n",
        "    transforms=test_augmentations\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "racegenders = metadata_df['racegender'].unique()\n",
        "\n",
        "# list to store indices of each racegender\n",
        "racegender_data_indices = {racegender: [] for racegender in racegenders}\n",
        "\n",
        "# Populate the list with indices of paired_files belonging to each racegender\n",
        "for idx, (_, _, racegender) in enumerate(train_set):\n",
        "    racegender_data_indices[racegender].append(idx)\n",
        "\n",
        "# list to store the interleaved indices\n",
        "interleaved_indices = []\n",
        "\n",
        "# Interleave indices based on racegenders\n",
        "for _ in range(len(train_set) // len(racegenders)):\n",
        "    for racegender in racegenders:\n",
        "        interleaved_indices.extend(racegender_data_indices[racegender])\n",
        "\n",
        "# sampler using interleaved indices\n",
        "sampler = torch.utils.data.sampler.SubsetRandomSampler(interleaved_indices)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # custom sampler for DataLoader to achieve the racegender pattern\n",
        "    train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_iou_list = []\n",
        "\n",
        "    for batch_idx, (images, masks, _) in enumerate(train_loader):\n",
        "        if images is None:\n",
        "            continue\n",
        "\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks.argmax(dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        predicted_masks = torch.argmax(outputs, dim=1)\n",
        "        batch_iou = jaccard_score(\n",
        "            masks.argmax(dim=1).cpu().numpy().flatten(),\n",
        "            predicted_masks.cpu().numpy().flatten(),\n",
        "            average='micro'\n",
        "        )\n",
        "        epoch_iou_list.append(batch_iou)\n",
        "\n",
        "\n",
        "    epoch_iou_avg = np.mean(epoch_iou_list)\n",
        "\n",
        "    model.eval()\n",
        "    valid_iou_list = []\n",
        "\n",
        "    for batch_idx, (images, masks, racegender) in enumerate(valid_loader):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "        predicted_masks = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        valid_iou = jaccard_score(\n",
        "            masks.argmax(dim=1).cpu().numpy().flatten(),\n",
        "            predicted_masks.cpu().numpy().flatten(),\n",
        "            average='micro'\n",
        "        )\n",
        "\n",
        "        valid_iou_list.append(valid_iou)\n",
        "\n",
        "    valid_iou_avg = np.mean(valid_iou_list)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train IoU: {epoch_iou_avg:.4f} - Validation IoU: {valid_iou_avg:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}